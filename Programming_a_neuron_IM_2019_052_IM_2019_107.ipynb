{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123Rajani/Implementing-AND-OR-XOR-Gates/blob/main/Programming_a_neuron_IM_2019_052_IM_2019_107.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThZIpRsl6QEg"
      },
      "source": [
        "You are supposed to write a program to implement a single neuron-based machine learning system. Use the ADALINE architecture and its weight update formula to make your system. Train the system with input-output pairs of AND, OR and XOR gates and observe the outputs.\n",
        "\n",
        "IM/2019/107-Rajani Navoda\n",
        "IM/2019/052-Tharindu Adhikari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYbvBzBl30vg",
        "outputId": "4352d66d-bebf-47fa-a639-d9f8d507e923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "#### LEARNING AND GATE\n",
        "\n",
        "# import Python Libraries\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        " \n",
        "# definition of the sigmoidFunction Function\n",
        "def sigmoidFunction(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        " \n",
        "# Initialization of the neural network parameters.\n",
        "# constraints: weight range 0-1. \n",
        "# bies values =0.\n",
        "\n",
        "def NNparameters(input, HLnurons, output):\n",
        "    weight1 = np.random.randn(HLnurons, input)\n",
        "    weight2 = np.random.randn(output, HLnurons)\n",
        "    b1 = np.zeros((HLnurons, 1))\n",
        "    b2 = np.zeros((output, 1))\n",
        "     \n",
        "    parameters = {\"weight1\" : weight1, \"b1\": b1,\n",
        "                  \"weight2\" : weight2, \"b2\": b2}\n",
        "    return parameters\n",
        "\n",
        "\n",
        "# Forward Propagation\n",
        "def forwardPropagation(X, Y, parameters):\n",
        "    m = X.shape[1]\n",
        "    weight1 = parameters[\"weight1\"]\n",
        "    weight2 = parameters[\"weight2\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        " \n",
        "    Z1 = np.dot(weight1, X) + b1\n",
        "    A1 = sigmoidFunction(Z1)\n",
        "    Z2 = np.dot(weight2, A1) + b2\n",
        "    A2 = sigmoidFunction(Z2)\n",
        " \n",
        "    cache = (Z1, A1, weight1, b1, Z2, A2, weight2, b2)\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
        "    cost = -np.sum(logprobs) / m\n",
        "    return cost, cache, A2\n",
        " \n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "def backwardPropagation(X, Y, cache):\n",
        "    m = X.shape[1]\n",
        "    (Z1, A1, weight1, b1, Z2, A2, weight2, b2) = cache\n",
        "     \n",
        "    dZ2 = A2 - Y\n",
        "    dweight2 = np.dot(dZ2, A1.T) / m\n",
        "    db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
        "     \n",
        "    dA1 = np.dot(weight2.T, dZ2)\n",
        "    dZ1 = np.multiply(dA1, A1 * (1- A1))\n",
        "    dweight1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
        "     \n",
        "    gradients = {\"dZ2\": dZ2, \"dweight2\": dweight2, \"db2\": db2,\n",
        "                 \"dZ1\": dZ1, \"dweight1\": dweight1, \"db1\": db1}\n",
        "    return gradients\n",
        " \n",
        "\n",
        "# Updating the weights based on the negative gradients\n",
        "def updateParameters(parameters, gradients, learningRate):\n",
        "    parameters[\"weight1\"] = parameters[\"weight1\"] - learningRate * gradients[\"dweight1\"]\n",
        "    parameters[\"weight2\"] = parameters[\"weight2\"] - learningRate * gradients[\"dweight2\"]\n",
        "    parameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
        "    parameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
        "    return parameters\n",
        " \n",
        "\n",
        "# Model to learn the AND truth table\n",
        "\n",
        "\n",
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) #AND inputs\n",
        "Y = np.array([[0, 0, 0, 1]]) # AND output\n",
        " \n",
        "# Define model parameters\n",
        "HLnurons = 4 # number of hidden layer neurons (2)\n",
        "input = X.shape[0] # number of input features (2)\n",
        "output = Y.shape[0] # number of output features (1)\n",
        "parameters = NNparameters(input, HLnurons, output)\n",
        "epoch = 100000  ##number of learning cycles\n",
        "learningRate = 0.01\n",
        "losses = np.zeros((epoch, 1)) \n",
        " \n",
        "for i in range(epoch):\n",
        "    losses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)\n",
        "    gradients = backwardPropagation(X, Y, cache)\n",
        "    parameters = updateParameters(parameters, gradients, learningRate)\n",
        " \n",
        "# Testing1\n",
        "X = np.array([[1, 0, 0, 1], [0, 1, 0, 1]]) # AND input\n",
        "cost, _, A2 = forwardPropagation(X, Y, parameters)\n",
        "prediction = (A2 > 0.5) * 1.0\n",
        "# print(A2)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwU9Qt7C9BXm",
        "outputId": "8ad3c013-8d5b-468d-99b7-8d0fb58df946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 1. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "#### LEARNING OR GATE\n",
        "\n",
        "# import Python Libraries\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        " \n",
        "# definition of the sigmoidFunction Function\n",
        "def sigmoidFunction(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        " \n",
        "# Initialization of the neural network parameters.\n",
        "# constraints: weight range 0-1. \n",
        "# bies values =0.\n",
        "\n",
        "def NNparameters(input, HLnurons, output):\n",
        "    weight1 = np.random.randn(HLnurons, input)\n",
        "    weight2 = np.random.randn(output, HLnurons)\n",
        "    b1 = np.zeros((HLnurons, 1))\n",
        "    b2 = np.zeros((output, 1))\n",
        "     \n",
        "    parameters = {\"weight1\" : weight1, \"b1\": b1,\n",
        "                  \"weight2\" : weight2, \"b2\": b2}\n",
        "    return parameters\n",
        "\n",
        "\n",
        "# Forward Propagation\n",
        "def forwardPropagation(X, Y, parameters):\n",
        "    m = X.shape[1]\n",
        "    weight1 = parameters[\"weight1\"]\n",
        "    weight2 = parameters[\"weight2\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        " \n",
        "    Z1 = np.dot(weight1, X) + b1\n",
        "    A1 = sigmoidFunction(Z1)\n",
        "    Z2 = np.dot(weight2, A1) + b2\n",
        "    A2 = sigmoidFunction(Z2)\n",
        " \n",
        "    cache = (Z1, A1, weight1, b1, Z2, A2, weight2, b2)\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
        "    cost = -np.sum(logprobs) / m\n",
        "    return cost, cache, A2\n",
        " \n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "def backwardPropagation(X, Y, cache):\n",
        "    m = X.shape[1]\n",
        "    (Z1, A1, weight1, b1, Z2, A2, weight2, b2) = cache\n",
        "     \n",
        "    dZ2 = A2 - Y\n",
        "    dweight2 = np.dot(dZ2, A1.T) / m\n",
        "    db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
        "     \n",
        "    dA1 = np.dot(weight2.T, dZ2)\n",
        "    dZ1 = np.multiply(dA1, A1 * (1- A1))\n",
        "    dweight1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
        "     \n",
        "    gradients = {\"dZ2\": dZ2, \"dweight2\": dweight2, \"db2\": db2,\n",
        "                 \"dZ1\": dZ1, \"dweight1\": dweight1, \"db1\": db1}\n",
        "    return gradients\n",
        " \n",
        "\n",
        "# Updating the weights based on the negative gradients\n",
        "def updateParameters(parameters, gradients, learningRate):\n",
        "    parameters[\"weight1\"] = parameters[\"weight1\"] - learningRate * gradients[\"dweight1\"]\n",
        "    parameters[\"weight2\"] = parameters[\"weight2\"] - learningRate * gradients[\"dweight2\"]\n",
        "    parameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
        "    parameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
        "    return parameters\n",
        " \n",
        "\n",
        "# Model to learn the OR truth table\n",
        "\n",
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # OR inputs\n",
        "Y = np.array([[0, 1, 1, 1]]) # OR output\n",
        " \n",
        "# Define model parameters\n",
        "HLnurons = 4 # number of hidden layer neurons (2)\n",
        "input = X.shape[0] # number of input features (2)\n",
        "output = Y.shape[0] # number of output features (1)\n",
        "parameters = NNparameters(input, HLnurons, output)\n",
        "epoch = 100000  ##number of learning cycles\n",
        "learningRate = 0.01\n",
        "losses = np.zeros((epoch, 1)) \n",
        " \n",
        "for i in range(epoch):\n",
        "    losses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)\n",
        "    gradients = backwardPropagation(X, Y, cache)\n",
        "    parameters = updateParameters(parameters, gradients, learningRate)\n",
        " \n",
        "\n",
        "# Testing2\n",
        "X = np.array([[1, 0, 0, 1], [0, 1, 0, 1]]) # OR input\n",
        "cost, _, A2 = forwardPropagation(X, Y, parameters)\n",
        "prediction = (A2 > 0.5) * 1.0\n",
        "# print(A2)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXTJuxJr91L1",
        "outputId": "67750319-2026-4e98-b980-64162cab89e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "#### LEARNING XOR GATE\n",
        "\n",
        "# import Python Libraries\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        " \n",
        "# definition of the sigmoidFunction Function\n",
        "def sigmoidFunction(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        " \n",
        "# Initialization of the neural network parameters.\n",
        "# constraints: weight range 0-1. \n",
        "# bies values =0.\n",
        "\n",
        "def NNparameters(input, HLnurons, output):\n",
        "    weight1 = np.random.randn(HLnurons, input)\n",
        "    weight2 = np.random.randn(output, HLnurons)\n",
        "    b1 = np.zeros((HLnurons, 1))\n",
        "    b2 = np.zeros((output, 1))\n",
        "     \n",
        "    parameters = {\"weight1\" : weight1, \"b1\": b1,\n",
        "                  \"weight2\" : weight2, \"b2\": b2}\n",
        "    return parameters\n",
        "\n",
        "\n",
        "# Forward Propagation\n",
        "def forwardPropagation(X, Y, parameters):\n",
        "    m = X.shape[1]\n",
        "    weight1 = parameters[\"weight1\"]\n",
        "    weight2 = parameters[\"weight2\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        " \n",
        "    Z1 = np.dot(weight1, X) + b1\n",
        "    A1 = sigmoidFunction(Z1)\n",
        "    Z2 = np.dot(weight2, A1) + b2\n",
        "    A2 = sigmoidFunction(Z2)\n",
        " \n",
        "    cache = (Z1, A1, weight1, b1, Z2, A2, weight2, b2)\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
        "    cost = -np.sum(logprobs) / m\n",
        "    return cost, cache, A2\n",
        " \n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "def backwardPropagation(X, Y, cache):\n",
        "    m = X.shape[1]\n",
        "    (Z1, A1, weight1, b1, Z2, A2, weight2, b2) = cache\n",
        "     \n",
        "    dZ2 = A2 - Y\n",
        "    dweight2 = np.dot(dZ2, A1.T) / m\n",
        "    db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
        "     \n",
        "    dA1 = np.dot(weight2.T, dZ2)\n",
        "    dZ1 = np.multiply(dA1, A1 * (1- A1))\n",
        "    dweight1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
        "     \n",
        "    gradients = {\"dZ2\": dZ2, \"dweight2\": dweight2, \"db2\": db2,\n",
        "                 \"dZ1\": dZ1, \"dweight1\": dweight1, \"db1\": db1}\n",
        "    return gradients\n",
        " \n",
        "\n",
        "# Updating the weights based on the negative gradients\n",
        "def updateParameters(parameters, gradients, learningRate):\n",
        "    parameters[\"weight1\"] = parameters[\"weight1\"] - learningRate * gradients[\"dweight1\"]\n",
        "    parameters[\"weight2\"] = parameters[\"weight2\"] - learningRate * gradients[\"dweight2\"]\n",
        "    parameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
        "    parameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
        "    return parameters\n",
        " \n",
        "\n",
        "# Model to learn the OR truth table\n",
        "\n",
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # XOR inputs\n",
        "Y = np.array([[0, 1, 1, 0]]) # XOR output\n",
        " \n",
        "# Define model parameters\n",
        "HLnurons = 4 # number of hidden layer neurons (2)\n",
        "input = X.shape[0] # number of input features (2)\n",
        "output = Y.shape[0] # number of output features (1)\n",
        "parameters = NNparameters(input, HLnurons, output)\n",
        "epoch = 100000  ##number of learning cycles\n",
        "learningRate = 0.01\n",
        "losses = np.zeros((epoch, 1)) \n",
        " \n",
        "for i in range(epoch):\n",
        "    losses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)\n",
        "    gradients = backwardPropagation(X, Y, cache)\n",
        "    parameters = updateParameters(parameters, gradients, learningRate)\n",
        " \n",
        "\n",
        "# Testing3\n",
        "X = np.array([[1, 0, 0, 1], [0, 1, 0, 1]]) # XOR input\n",
        "cost, _, A2 = forwardPropagation(X, Y, parameters)\n",
        "prediction = (A2 > 0.5) * 1.0\n",
        "# print(A2)\n",
        "print(prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}